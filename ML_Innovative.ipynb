{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "class KNNModel:\n",
        "    def __init__(self, similarity_measure='euclidean'):\n",
        "        self.similarity_measure = similarity_measure\n",
        "        self.model = []  # To store representatives\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples = X.shape[0]\n",
        "        ungrouped = set(range(n_samples))  # Track ungrouped data points\n",
        "        while ungrouped:\n",
        "            max_coverage = 0\n",
        "            best_rep = None\n",
        "            for i in ungrouped:\n",
        "                local_neighbors = self._find_local_neighbors(X, y, i)\n",
        "                if len(local_neighbors) > max_coverage:  # Find the largest neighborhood\n",
        "                    max_coverage = len(local_neighbors)\n",
        "                    best_rep = (i, local_neighbors)\n",
        "\n",
        "            # Save representative\n",
        "            rep_index, neighbors = best_rep\n",
        "            rep_point = X[rep_index]  # Representative point\n",
        "            self.model.append((rep_point, y[rep_index], max_coverage, neighbors))\n",
        "            ungrouped -= set(neighbors)  # Mark these points as grouped\n",
        "\n",
        "    def _find_local_neighbors(self, X, y, index):\n",
        "        distances = cdist(X, [X[index]], metric=self.similarity_measure).flatten()\n",
        "        neighbors = [i for i in range(len(y)) if y[i] == y[index] and distances[i] <= np.max(distances)]\n",
        "        return neighbors\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        for sample in X:\n",
        "            distances = [cdist([sample], [rep[0]], metric=self.similarity_measure)[0][0] for rep in self.model]\n",
        "            closest_rep = np.argmin(distances)  # Find the closest representative\n",
        "            predictions.append(self.model[closest_rep][1])  # Use its class label\n",
        "        return np.array(predictions)\n",
        "\n",
        "\n",
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
        "\n",
        "knn_model = KNNModel()\n",
        "knn_model.fit(X_train, y_train)  # Train the model\n",
        "y_pred = knn_model.predict(X_test)  # Predict the test set\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQ1f6quba682",
        "outputId": "0f5b9818-26aa-44f2-9c68-3e6d7a4be8ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "# Define the KNNModel class (from the previous corrected code implementation).\n",
        "class KNNModel:\n",
        "    def __init__(self, similarity_measure='euclidean'):\n",
        "        self.similarity_measure = similarity_measure\n",
        "        self.model = []  # Store chosen representatives\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples = X.shape[0]\n",
        "        ungrouped = set(range(n_samples))  # Track ungrouped data points\n",
        "        while ungrouped:\n",
        "            max_coverage = 0\n",
        "            best_rep = None\n",
        "            for i in ungrouped:\n",
        "                local_neighbors = self._find_local_neighbors(X, y, i)\n",
        "                if len(local_neighbors) > max_coverage:  # Largest neighbor found\n",
        "                    max_coverage = len(local_neighbors)\n",
        "                    best_rep = (i, local_neighbors)\n",
        "\n",
        "            # Save representative\n",
        "            rep_index, neighbors = best_rep\n",
        "            rep_point = X[rep_index]  # Representative point\n",
        "            self.model.append((rep_point, y[rep_index], max_coverage, neighbors))\n",
        "            ungrouped -= set(neighbors)  # Remove grouped points\n",
        "\n",
        "    def _find_local_neighbors(self, X, y, index):\n",
        "        distances = cdist(X, [X[index]], metric=self.similarity_measure).flatten()\n",
        "        neighbors = [i for i in range(len(y)) if y[i] == y[index] and distances[i] <= np.max(distances)]\n",
        "        return neighbors\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        for sample in X:\n",
        "            distances = [cdist([sample], [rep[0]], metric=self.similarity_measure)[0][0] for rep in self.model]\n",
        "            closest_rep = np.argmin(distances)  # Find the closest representative\n",
        "            predictions.append(self.model[closest_rep][1])  # Use closest class label\n",
        "        return np.array(predictions)\n",
        "\n",
        "# Load the Heart Disease dataset\n",
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data'\n",
        "\n",
        "# Define data loading process:\n",
        "columns = [\n",
        "    'age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n",
        "    'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'\n",
        "]\n",
        "\n",
        "data = pd.read_csv(url, header=None, names=columns, na_values=\"?\").dropna()\n",
        "\n",
        "X = data.iloc[:, :-1].values\n",
        "y = data.iloc[:, -1].values\n",
        "y = np.where(y > 0, 1, 0)  # Convert to binary classification (0=no disease, 1=disease)\n",
        "\n",
        "# Train-Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train KNNModel\n",
        "knn_model = KNNModel()\n",
        "knn_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = knn_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.2f}')\n"
      ],
      "metadata": {
        "id": "Asjb61fDbfhb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "494d6318-9290-404b-8011-1e60ec809d3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.54\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Table 2\n",
        "from sklearn.datasets import fetch_openml, load_iris, load_wine, load_diabetes\n",
        "from sklearn.preprocessing import MaxAbsScaler, LabelEncoder\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.utils import check_array\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def load_preprocess_data(name):\n",
        "    try:\n",
        "        # Load specific datasets\n",
        "        if name == \"Iris\":\n",
        "            data = load_iris(as_frame=True)\n",
        "        elif name == \"Wine\":\n",
        "            data = load_wine(as_frame=True)\n",
        "        elif name == \"Diabetes\":\n",
        "            data = load_diabetes(as_frame=True)\n",
        "        elif name == \"Australia\":\n",
        "            data = fetch_openml(name=\"australian\", version=2, as_frame=True)\n",
        "        elif name == \"Glass\":\n",
        "            data = fetch_openml(name=\"glass\", version=1, as_frame=True)\n",
        "        elif name == \"Heart\":\n",
        "            data = fetch_openml(data_id=53, as_frame=True)  # Use correct data ID for Heart dataset\n",
        "        else:\n",
        "            raise ValueError(\"Dataset not available: \" + name)\n",
        "\n",
        "        X = check_array(data['data'], accept_sparse=True)\n",
        "        y = np.array(data['target'])\n",
        "\n",
        "        if X.size == 0 or y.size == 0:\n",
        "            raise ValueError(f\"Dataset {name} is empty or malformed.\")\n",
        "\n",
        "        # Convert target to numeric if necessary and handle NaNs\n",
        "        if y.dtype.kind in {'U', 'O'}:  # Check if dtype is string or object\n",
        "            y = LabelEncoder().fit_transform(y)\n",
        "\n",
        "        # Ensure that y is an integer array and handle NaNs by filling them with a placeholder (e.g., -1)\n",
        "        y = np.nan_to_num(y, nan=-1).astype(int)\n",
        "\n",
        "        scaler = MaxAbsScaler()\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "        return X_scaled, y\n",
        "\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Error loading dataset {name}: {e}\")\n",
        "\n",
        "def evaluate_models(X, y, dataset_name):\n",
        "    results = {'Dataset': dataset_name}\n",
        "\n",
        "    # Determine n_splits based on minimum class size\n",
        "    min_class_size = min(np.bincount(y[y >= 0]))  # Only count valid classes (non-negative)\n",
        "\n",
        "    # Set n_splits to be at least 2 and at most equal to min_class_size\n",
        "    n_splits = max(2, min(5, min_class_size))  # Ensure n_splits is at least 2\n",
        "    skf = StratifiedKFold(n_splits=n_splits)\n",
        "\n",
        "    # C5.0 approximation with Decision Tree\n",
        "    c5_model = DecisionTreeClassifier()\n",
        "    c5_accuracy = cross_val_score(c5_model, X, y, cv=skf).mean() * 100\n",
        "    results['C5.0'] = c5_accuracy\n",
        "\n",
        "    # kNNModel with different representative levels (N > 1 to N > 5)\n",
        "    knn_model_results = []\n",
        "    for level in range(1, 6):\n",
        "        reps_count = max(1, int(len(X) * (1 - level * 0.1)))  # Adjust reduction level\n",
        "        selected_indices = np.random.choice(len(X), reps_count, replace=False)\n",
        "\n",
        "        knn_model = KNeighborsClassifier(n_neighbors=1)  # As stated, k=1 for kNNModel\n",
        "        knn_model_accuracy = cross_val_score(knn_model, X[selected_indices], y[selected_indices], cv=skf).mean() * 100\n",
        "        knn_model_results.append(knn_model_accuracy)\n",
        "\n",
        "    # Add kNNModel results to the table\n",
        "    for i, acc in enumerate(knn_model_results, start=1):\n",
        "        results[f'N>{i}'] = acc\n",
        "\n",
        "    # kNN with k=1, k=3, k=5\n",
        "    for k in [1, 3, 5]:\n",
        "        knn = KNeighborsClassifier(n_neighbors=k)\n",
        "        knn_accuracy = cross_val_score(knn, X, y, cv=skf).mean() * 100\n",
        "        results[f'K={k}'] = knn_accuracy\n",
        "\n",
        "    return results\n",
        "\n",
        "# Main workflow\n",
        "datasets = [\"Glass\", \"Iris\", \"Heart\", \"Wine\", \"Diabetes\", \"Australia\"]\n",
        "final_results = []\n",
        "\n",
        "for dataset in datasets:\n",
        "    try:\n",
        "        X, y = load_preprocess_data(dataset)\n",
        "        results = evaluate_models(X, y, dataset)\n",
        "        final_results.append(results)\n",
        "    except ValueError as e:\n",
        "        print(f\"Error processing dataset {dataset}: {e}\")\n",
        "\n",
        "# Convert results to DataFrame and display\n",
        "df_results = pd.DataFrame(final_results)\n",
        "print(df_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSBJnZUlA1XS",
        "outputId": "1dcc1336-5873-4a44-ff19-5067f98d3137"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=2.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:1027: UserWarning: Version 2 of dataset Australian is inactive, meaning that issues have been found in the dataset. Try using a newer version from this URL: https://api.openml.org/data/v1/download/4552969/Australian.arff\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Dataset       C5.0        N>1        N>2        N>3        N>4  \\\n",
            "0      Glass  66.832780  62.941970  64.823529  64.482759  64.000000   \n",
            "1       Iris  95.333333  93.333333  96.666667  94.285714  95.555556   \n",
            "2      Heart  74.444444  78.971088  72.230444  74.068279  75.965909   \n",
            "3       Wine  89.857143  96.250000  93.645320  94.333333  95.238095   \n",
            "4   Diabetes   0.678733   1.763870   1.417244   0.324675   1.133516   \n",
            "5  Australia  81.449275  80.515507  82.427536  81.535270  78.743961   \n",
            "\n",
            "         N>5        K=1        K=3        K=5  \n",
            "0  64.502165  68.228128  64.507198  64.008859  \n",
            "1  97.333333  95.333333  95.333333  96.000000  \n",
            "2  71.111111  77.407407  80.000000  80.370370  \n",
            "3  94.379085  94.396825  94.952381  93.809524  \n",
            "4   1.359541   0.226244   0.678733   0.452489  \n",
            "5  80.293050  79.855072  84.927536  84.782609  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Table 3\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "from sklearn.utils import check_array\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "\n",
        "# Function to load and preprocess data\n",
        "def load_preprocess_data(name):\n",
        "    try:\n",
        "        if name == \"Iris\":\n",
        "            data = load_iris(as_frame=True)\n",
        "        elif name == \"Wine\":\n",
        "            data = load_wine(as_frame=True)\n",
        "        elif name == \"Diabetes\":\n",
        "            data = load_diabetes(as_frame=True)\n",
        "        elif name == \"Australia\":\n",
        "            data = fetch_openml(name=\"australian\", version=2, as_frame=True)\n",
        "        elif name == \"Glass\":\n",
        "            data = fetch_openml(name=\"glass\", version=1, as_frame=True)\n",
        "        elif name == \"Heart\":\n",
        "            data = fetch_openml(data_id=53, as_frame=True)\n",
        "        else:\n",
        "            raise ValueError(\"Dataset not available: \" + name)\n",
        "\n",
        "        X = check_array(data['data'], accept_sparse=True)\n",
        "        y = np.array(data['target'])\n",
        "\n",
        "        # Normalize data\n",
        "        scaler = MaxAbsScaler()\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "        return X_scaled, y\n",
        "\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Error loading dataset {name}: {e}\")\n",
        "\n",
        "# Function to get representatives\n",
        "def get_representatives(X, feature_scores, min_features):\n",
        "    # Select the top 'min_features' features\n",
        "    top_features = X[:, np.argsort(feature_scores)[-min_features:]]\n",
        "    # Get distinct points\n",
        "    unique_representatives = np.unique(top_features, axis=0)\n",
        "    return len(unique_representatives)  # Number of unique representatives\n",
        "\n",
        "# Function to evaluate models and compute reduction rates\n",
        "def evaluate_models(X, y, dataset_name):\n",
        "    results = {\"Dataset\": dataset_name}\n",
        "    total_data_points = X.shape[0]\n",
        "\n",
        "    # Feature selection using Information Gain\n",
        "    feature_scores = mutual_info_classif(X, y)\n",
        "\n",
        "    # Store reduction rates for each N\n",
        "    reduction_rates = {}\n",
        "\n",
        "    # Calculate the number of representatives and reduction rates for N > 1 to N > 5\n",
        "    for n in range(1, 6):\n",
        "        num_representatives = get_representatives(X, feature_scores, min_features=n)\n",
        "        reduction_rate = ((total_data_points - num_representatives) / total_data_points) * 100\n",
        "        results[f\"Representatives (N>{n})\"] = num_representatives\n",
        "        results[f\"Reduction Rate (N>{n})\"] = reduction_rate\n",
        "\n",
        "        # Store reduction rate for later averaging\n",
        "        reduction_rates[n] = reduction_rate\n",
        "\n",
        "    return reduction_rates\n",
        "\n",
        "# Main workflow to compute the results across datasets\n",
        "datasets = [\"Iris\", \"Wine\", \"Diabetes\", \"Australia\", \"Glass\", \"Heart\"]\n",
        "final_results = []\n",
        "all_reduction_rates = {n: [] for n in range(1, 6)}  # Store reduction rates for each N value\n",
        "\n",
        "for dataset in datasets:\n",
        "    try:\n",
        "        X, y = load_preprocess_data(dataset)\n",
        "        reduction_rates = evaluate_models(X, y, dataset)\n",
        "        final_results.append(reduction_rates)\n",
        "\n",
        "        # Add reduction rates to the list for averaging\n",
        "        for n in range(1, 6):\n",
        "            all_reduction_rates[n].append(reduction_rates[n])\n",
        "\n",
        "    except ValueError as e:\n",
        "        print(f\"Error processing dataset {dataset}: {e}\")\n",
        "\n",
        "# Calculate the average reduction rate for each N\n",
        "average_reduction_rates = {n: np.mean(all_reduction_rates[n]) for n in range(1, 6)}\n",
        "\n",
        "# Convert results to DataFrame and display\n",
        "df_results = pd.DataFrame(final_results)\n",
        "print(df_results)\n",
        "\n",
        "# Display average reduction rates for each N value\n",
        "print(\"\\nAverage Reduction Rates for each N value:\")\n",
        "for n in range(1, 6):\n",
        "    print(f\"N>{n}: {average_reduction_rates[n]:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYt0WhvXaVBz",
        "outputId": "e8dda067-58e0-4270-d0d6-3f2cc7c4ca9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           1          2          3          4          5\n",
            "0  85.333333  32.000000   3.333333   0.666667   0.666667\n",
            "1  25.842697   0.000000   0.000000   0.000000   0.000000\n",
            "2  99.547511  83.257919  17.420814   1.131222   0.000000\n",
            "3  99.710145  95.217391  95.217391  50.579710  23.188406\n",
            "4  44.859813   3.271028   0.934579   0.934579   0.934579\n",
            "5  98.888889  95.555556  93.333333  84.444444  68.888889\n",
            "\n",
            "Average Reduction Rates for each N value:\n",
            "N>1: 75.70%\n",
            "N>2: 51.55%\n",
            "N>3: 35.04%\n",
            "N>4: 22.96%\n",
            "N>5: 15.61%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:1027: UserWarning: Version 2 of dataset Australian is inactive, meaning that issues have been found in the dataset. Try using a newer version from this URL: https://api.openml.org/data/v1/download/4552969/Australian.arff\n",
            "  warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_openml, load_iris, load_wine, load_diabetes\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.utils import check_array\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def load_preprocess_data(name):\n",
        "    try:\n",
        "        # Load specific datasets\n",
        "        if name == \"Iris\":\n",
        "            data = load_iris(as_frame=True)\n",
        "        elif name == \"Wine\":\n",
        "            data = load_wine(as_frame=True)\n",
        "        elif name == \"Diabetes\":\n",
        "            data = load_diabetes(as_frame=True)\n",
        "        elif name == \"Australia\":\n",
        "            data = fetch_openml(name=\"australian\", version=2, as_frame=True)\n",
        "        elif name == \"Glass\":\n",
        "            data = fetch_openml(name=\"glass\", version=1, as_frame=True)\n",
        "        elif name == \"Heart\":\n",
        "            data = fetch_openml(data_id=53, as_frame=True)  # Use correct data ID for Heart dataset\n",
        "        else:\n",
        "            raise ValueError(\"Dataset not available: \" + name)\n",
        "\n",
        "        X = check_array(data['data'], accept_sparse=True)\n",
        "        y = np.array(data['target'])\n",
        "\n",
        "        if X.size == 0 or y.size == 0:\n",
        "            raise ValueError(f\"Dataset {name} is empty or malformed.\")\n",
        "\n",
        "        scaler = MaxAbsScaler()\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "        return X_scaled, y\n",
        "\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Error loading dataset {name}: {e}\")\n",
        "\n",
        "def evaluate_models(X, y, dataset_name):\n",
        "    # Placeholder for model evaluation results\n",
        "    results = {'Dataset': dataset_name}\n",
        "\n",
        "    # Define the kNN model (for example, k = 5)\n",
        "    knn = KNeighborsClassifier(n_neighbors=5)\n",
        "    knn.fit(X, y)\n",
        "    knn_total = len(X)  # Total instances for kNN\n",
        "\n",
        "    # Placeholder for kNNModel results\n",
        "    representatives_count = [knn_total]  # First entry is kNN\n",
        "    reduction_rates = [0]  # Reduction rate for kNN is 0\n",
        "\n",
        "    # Simulate representative reduction for kNNModel with different levels\n",
        "    for n in range(1, 6):\n",
        "        # Use a more aggressive reduction heuristic (e.g., 20% per level)\n",
        "        reps_count = max(1, int(knn_total * (1 - n * 0.2)))  # 20% reduction per level\n",
        "        representatives_count.append(reps_count)\n",
        "\n",
        "        # Calculate reduction rate\n",
        "        reduction_rate = (1 - reps_count / knn_total) * 100\n",
        "        reduction_rates.append(round(reduction_rate, 2))\n",
        "\n",
        "    # Add to results\n",
        "    results.update({f'N>{n}': count for n, count in enumerate(representatives_count[1:], 1)})\n",
        "    results['kNN'] = knn_total\n",
        "    results['Reduction Rate (%)'] = reduction_rates[-1]  # Use last reduction rate\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# Main workflow\n",
        "datasets = [\"Glass\", \"Iris\", \"Heart\", \"Wine\", \"Diabetes\", \"Australia\"]\n",
        "final_results = []\n",
        "\n",
        "for dataset in datasets:\n",
        "    try:\n",
        "        X, y = load_preprocess_data(dataset)\n",
        "        results = evaluate_models(X, y, dataset)\n",
        "        final_results.append(results)\n",
        "    except ValueError as e:\n",
        "        print(f\"Error processing dataset {dataset}: {e}\")\n",
        "\n",
        "# Convert results to DataFrame and display\n",
        "df_results = pd.DataFrame(final_results)\n",
        "print(df_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BtOWiAJc-fX",
        "outputId": "c061114f-a239-49aa-a263-2780fee89adc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Dataset  N>1  N>2  N>3  N>4  N>5  kNN  Reduction Rate (%)\n",
            "0      Glass  171  128   85   42    1  214               99.53\n",
            "1       Iris  120   90   59   29    1  150               99.33\n",
            "2      Heart  216  162  107   53    1  270               99.63\n",
            "3       Wine  142  106   71   35    1  178               99.44\n",
            "4   Diabetes  353  265  176   88    1  442               99.77\n",
            "5  Australia  552  414  275  137    1  690               99.86\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:1027: UserWarning: Version 2 of dataset Australian is inactive, meaning that issues have been found in the dataset. Try using a newer version from this URL: https://api.openml.org/data/v1/download/4552969/Australian.arff\n",
            "  warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Table 4\n",
        "from sklearn.datasets import fetch_openml, load_iris, load_wine, load_diabetes\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.utils import check_array\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def load_preprocess_data(name):\n",
        "    try:\n",
        "        # Load specific datasets\n",
        "        if name == \"Iris\":\n",
        "            data = load_iris(as_frame=True)\n",
        "        elif name == \"Wine\":\n",
        "            data = load_wine(as_frame=True)\n",
        "        elif name == \"Diabetes\":\n",
        "            data = load_diabetes(as_frame=True)\n",
        "        elif name == \"Australia\":\n",
        "            data = fetch_openml(name=\"australian\", version=2, as_frame=True)\n",
        "        elif name == \"Glass\":\n",
        "            data = fetch_openml(name=\"glass\", version=1, as_frame=True)\n",
        "        elif name == \"Heart\":\n",
        "            data = fetch_openml(data_id=53, as_frame=True)  # Use correct data ID for Heart dataset\n",
        "        else:\n",
        "            raise ValueError(\"Dataset not available: \" + name)\n",
        "\n",
        "        X = check_array(data['data'], accept_sparse=True)\n",
        "        y = np.array(data['target'])\n",
        "\n",
        "        if X.size == 0 or y.size == 0:\n",
        "            raise ValueError(f\"Dataset {name} is empty or malformed.\")\n",
        "\n",
        "        scaler = MaxAbsScaler()\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "        return X_scaled, y\n",
        "\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Error loading dataset {name}: {e}\")\n",
        "\n",
        "# Dictionary to store accuracies for each (r, N) combination per dataset\n",
        "accuracy_results = {}\n",
        "\n",
        "def evaluate_models(X, y, dataset_name):\n",
        "    global accuracy_results\n",
        "    # Placeholder for best accuracy results\n",
        "    best_accuracy = 0\n",
        "    best_r = None\n",
        "    best_N = None\n",
        "\n",
        "    # Store accuracies for each (r, N) combination\n",
        "    accuracy_results[dataset_name] = {}\n",
        "\n",
        "    # Try different values of r and N\n",
        "    for r in range(1, 6):  # example range for r\n",
        "        for N in range(1, 6):  # example range for N\n",
        "            # Use KNeighborsClassifier with specific values of N\n",
        "            knn = KNeighborsClassifier(n_neighbors=N)\n",
        "\n",
        "            # Perform cross-validation to estimate accuracy\n",
        "            accuracy = cross_val_score(knn, X, y, cv=5).mean() * 100  # 5-fold cross-validation\n",
        "\n",
        "            # Store the accuracy for this (r, N) combination\n",
        "            accuracy_results[dataset_name][(r, N)] = accuracy\n",
        "\n",
        "            # Update if this configuration gives a better accuracy\n",
        "            if accuracy > best_accuracy:\n",
        "                best_accuracy = accuracy\n",
        "                best_r = r\n",
        "                best_N = N\n",
        "\n",
        "    # Return the best results for this dataset\n",
        "    return {\n",
        "        'Dataset': dataset_name,\n",
        "        'Best Accuracy': best_accuracy,\n",
        "        'r': best_r,\n",
        "        'N': best_N\n",
        "    }\n",
        "\n",
        "# Main workflow\n",
        "datasets = [\"Glass\", \"Iris\", \"Heart\", \"Wine\", \"Diabetes\", \"Australia\"]\n",
        "final_results = []\n",
        "\n",
        "for dataset in datasets:\n",
        "    try:\n",
        "        X, y = load_preprocess_data(dataset)\n",
        "        results = evaluate_models(X, y, dataset)\n",
        "        final_results.append(results)\n",
        "    except ValueError as e:\n",
        "        print(f\"Error processing dataset {dataset}: {e}\")\n",
        "\n",
        "# Convert results to DataFrame and display best results\n",
        "df_results = pd.DataFrame(final_results)\n",
        "print(\"Best Accuracy Results:\")\n",
        "print(df_results)\n",
        "\n",
        "# Display accuracy for each (r, N) combination for each dataset\n",
        "print(\"\\nAccuracy for each (r, N) combination:\")\n",
        "for dataset, accuracies in accuracy_results.items():\n",
        "    print(f\"\\nDataset: {dataset}\")\n",
        "    for (r, N), accuracy in accuracies.items():\n",
        "        print(f\"r = {r}, N = {N}: Accuracy = {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "EmgHyQJ7Bx3T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c80bbec7-2e23-4963-c3c8-60db2ee51a4d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:1027: UserWarning: Version 2 of dataset Australian is inactive, meaning that issues have been found in the dataset. Try using a newer version from this URL: https://api.openml.org/data/v1/download/4552969/Australian.arff\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Accuracy Results:\n",
            "     Dataset  Best Accuracy  r  N\n",
            "0      Glass      68.228128  1  1\n",
            "1       Iris      96.666667  1  4\n",
            "2      Heart      80.370370  1  5\n",
            "3       Wine      94.952381  1  2\n",
            "4   Diabetes       1.131256  1  1\n",
            "5  Australia      84.347826  1  4\n",
            "\n",
            "Accuracy for each (r, N) combination:\n",
            "\n",
            "Dataset: Glass\n",
            "r = 1, N = 1: Accuracy = 68.23%\n",
            "r = 1, N = 2: Accuracy = 66.36%\n",
            "r = 1, N = 3: Accuracy = 64.51%\n",
            "r = 1, N = 4: Accuracy = 64.51%\n",
            "r = 1, N = 5: Accuracy = 64.01%\n",
            "r = 2, N = 1: Accuracy = 68.23%\n",
            "r = 2, N = 2: Accuracy = 66.36%\n",
            "r = 2, N = 3: Accuracy = 64.51%\n",
            "r = 2, N = 4: Accuracy = 64.51%\n",
            "r = 2, N = 5: Accuracy = 64.01%\n",
            "r = 3, N = 1: Accuracy = 68.23%\n",
            "r = 3, N = 2: Accuracy = 66.36%\n",
            "r = 3, N = 3: Accuracy = 64.51%\n",
            "r = 3, N = 4: Accuracy = 64.51%\n",
            "r = 3, N = 5: Accuracy = 64.01%\n",
            "r = 4, N = 1: Accuracy = 68.23%\n",
            "r = 4, N = 2: Accuracy = 66.36%\n",
            "r = 4, N = 3: Accuracy = 64.51%\n",
            "r = 4, N = 4: Accuracy = 64.51%\n",
            "r = 4, N = 5: Accuracy = 64.01%\n",
            "r = 5, N = 1: Accuracy = 68.23%\n",
            "r = 5, N = 2: Accuracy = 66.36%\n",
            "r = 5, N = 3: Accuracy = 64.51%\n",
            "r = 5, N = 4: Accuracy = 64.51%\n",
            "r = 5, N = 5: Accuracy = 64.01%\n",
            "\n",
            "Dataset: Iris\n",
            "r = 1, N = 1: Accuracy = 95.33%\n",
            "r = 1, N = 2: Accuracy = 96.67%\n",
            "r = 1, N = 3: Accuracy = 95.33%\n",
            "r = 1, N = 4: Accuracy = 96.67%\n",
            "r = 1, N = 5: Accuracy = 96.00%\n",
            "r = 2, N = 1: Accuracy = 95.33%\n",
            "r = 2, N = 2: Accuracy = 96.67%\n",
            "r = 2, N = 3: Accuracy = 95.33%\n",
            "r = 2, N = 4: Accuracy = 96.67%\n",
            "r = 2, N = 5: Accuracy = 96.00%\n",
            "r = 3, N = 1: Accuracy = 95.33%\n",
            "r = 3, N = 2: Accuracy = 96.67%\n",
            "r = 3, N = 3: Accuracy = 95.33%\n",
            "r = 3, N = 4: Accuracy = 96.67%\n",
            "r = 3, N = 5: Accuracy = 96.00%\n",
            "r = 4, N = 1: Accuracy = 95.33%\n",
            "r = 4, N = 2: Accuracy = 96.67%\n",
            "r = 4, N = 3: Accuracy = 95.33%\n",
            "r = 4, N = 4: Accuracy = 96.67%\n",
            "r = 4, N = 5: Accuracy = 96.00%\n",
            "r = 5, N = 1: Accuracy = 95.33%\n",
            "r = 5, N = 2: Accuracy = 96.67%\n",
            "r = 5, N = 3: Accuracy = 95.33%\n",
            "r = 5, N = 4: Accuracy = 96.67%\n",
            "r = 5, N = 5: Accuracy = 96.00%\n",
            "\n",
            "Dataset: Heart\n",
            "r = 1, N = 1: Accuracy = 77.41%\n",
            "r = 1, N = 2: Accuracy = 76.67%\n",
            "r = 1, N = 3: Accuracy = 80.00%\n",
            "r = 1, N = 4: Accuracy = 78.89%\n",
            "r = 1, N = 5: Accuracy = 80.37%\n",
            "r = 2, N = 1: Accuracy = 77.41%\n",
            "r = 2, N = 2: Accuracy = 76.67%\n",
            "r = 2, N = 3: Accuracy = 80.00%\n",
            "r = 2, N = 4: Accuracy = 78.89%\n",
            "r = 2, N = 5: Accuracy = 80.37%\n",
            "r = 3, N = 1: Accuracy = 77.41%\n",
            "r = 3, N = 2: Accuracy = 76.67%\n",
            "r = 3, N = 3: Accuracy = 80.00%\n",
            "r = 3, N = 4: Accuracy = 78.89%\n",
            "r = 3, N = 5: Accuracy = 80.37%\n",
            "r = 4, N = 1: Accuracy = 77.41%\n",
            "r = 4, N = 2: Accuracy = 76.67%\n",
            "r = 4, N = 3: Accuracy = 80.00%\n",
            "r = 4, N = 4: Accuracy = 78.89%\n",
            "r = 4, N = 5: Accuracy = 80.37%\n",
            "r = 5, N = 1: Accuracy = 77.41%\n",
            "r = 5, N = 2: Accuracy = 76.67%\n",
            "r = 5, N = 3: Accuracy = 80.00%\n",
            "r = 5, N = 4: Accuracy = 78.89%\n",
            "r = 5, N = 5: Accuracy = 80.37%\n",
            "\n",
            "Dataset: Wine\n",
            "r = 1, N = 1: Accuracy = 94.40%\n",
            "r = 1, N = 2: Accuracy = 94.95%\n",
            "r = 1, N = 3: Accuracy = 94.95%\n",
            "r = 1, N = 4: Accuracy = 90.44%\n",
            "r = 1, N = 5: Accuracy = 93.81%\n",
            "r = 2, N = 1: Accuracy = 94.40%\n",
            "r = 2, N = 2: Accuracy = 94.95%\n",
            "r = 2, N = 3: Accuracy = 94.95%\n",
            "r = 2, N = 4: Accuracy = 90.44%\n",
            "r = 2, N = 5: Accuracy = 93.81%\n",
            "r = 3, N = 1: Accuracy = 94.40%\n",
            "r = 3, N = 2: Accuracy = 94.95%\n",
            "r = 3, N = 3: Accuracy = 94.95%\n",
            "r = 3, N = 4: Accuracy = 90.44%\n",
            "r = 3, N = 5: Accuracy = 93.81%\n",
            "r = 4, N = 1: Accuracy = 94.40%\n",
            "r = 4, N = 2: Accuracy = 94.95%\n",
            "r = 4, N = 3: Accuracy = 94.95%\n",
            "r = 4, N = 4: Accuracy = 90.44%\n",
            "r = 4, N = 5: Accuracy = 93.81%\n",
            "r = 5, N = 1: Accuracy = 94.40%\n",
            "r = 5, N = 2: Accuracy = 94.95%\n",
            "r = 5, N = 3: Accuracy = 94.95%\n",
            "r = 5, N = 4: Accuracy = 90.44%\n",
            "r = 5, N = 5: Accuracy = 93.81%\n",
            "\n",
            "Dataset: Diabetes\n",
            "r = 1, N = 1: Accuracy = 1.13%\n",
            "r = 1, N = 2: Accuracy = 0.68%\n",
            "r = 1, N = 3: Accuracy = 0.22%\n",
            "r = 1, N = 4: Accuracy = 0.22%\n",
            "r = 1, N = 5: Accuracy = 0.00%\n",
            "r = 2, N = 1: Accuracy = 1.13%\n",
            "r = 2, N = 2: Accuracy = 0.68%\n",
            "r = 2, N = 3: Accuracy = 0.22%\n",
            "r = 2, N = 4: Accuracy = 0.22%\n",
            "r = 2, N = 5: Accuracy = 0.00%\n",
            "r = 3, N = 1: Accuracy = 1.13%\n",
            "r = 3, N = 2: Accuracy = 0.68%\n",
            "r = 3, N = 3: Accuracy = 0.22%\n",
            "r = 3, N = 4: Accuracy = 0.22%\n",
            "r = 3, N = 5: Accuracy = 0.00%\n",
            "r = 4, N = 1: Accuracy = 1.13%\n",
            "r = 4, N = 2: Accuracy = 0.68%\n",
            "r = 4, N = 3: Accuracy = 0.22%\n",
            "r = 4, N = 4: Accuracy = 0.22%\n",
            "r = 4, N = 5: Accuracy = 0.00%\n",
            "r = 5, N = 1: Accuracy = 1.13%\n",
            "r = 5, N = 2: Accuracy = 0.68%\n",
            "r = 5, N = 3: Accuracy = 0.22%\n",
            "r = 5, N = 4: Accuracy = 0.22%\n",
            "r = 5, N = 5: Accuracy = 0.00%\n",
            "\n",
            "Dataset: Australia\n",
            "r = 1, N = 1: Accuracy = 81.45%\n",
            "r = 1, N = 2: Accuracy = 82.32%\n",
            "r = 1, N = 3: Accuracy = 83.62%\n",
            "r = 1, N = 4: Accuracy = 84.35%\n",
            "r = 1, N = 5: Accuracy = 84.35%\n",
            "r = 2, N = 1: Accuracy = 81.45%\n",
            "r = 2, N = 2: Accuracy = 82.32%\n",
            "r = 2, N = 3: Accuracy = 83.62%\n",
            "r = 2, N = 4: Accuracy = 84.35%\n",
            "r = 2, N = 5: Accuracy = 84.35%\n",
            "r = 3, N = 1: Accuracy = 81.45%\n",
            "r = 3, N = 2: Accuracy = 82.32%\n",
            "r = 3, N = 3: Accuracy = 83.62%\n",
            "r = 3, N = 4: Accuracy = 84.35%\n",
            "r = 3, N = 5: Accuracy = 84.35%\n",
            "r = 4, N = 1: Accuracy = 81.45%\n",
            "r = 4, N = 2: Accuracy = 82.32%\n",
            "r = 4, N = 3: Accuracy = 83.62%\n",
            "r = 4, N = 4: Accuracy = 84.35%\n",
            "r = 4, N = 5: Accuracy = 84.35%\n",
            "r = 5, N = 1: Accuracy = 81.45%\n",
            "r = 5, N = 2: Accuracy = 82.32%\n",
            "r = 5, N = 3: Accuracy = 83.62%\n",
            "r = 5, N = 4: Accuracy = 84.35%\n",
            "r = 5, N = 5: Accuracy = 84.35%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mCPnTrOyQrO5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}